# Entendiendo las Diferencias Fundamentales: CPU vs. GPU

Este documento resume mis notas y conclusiones sobre las diferencias arquitect√≥nicas y de casos de uso entre una Unidad Central de Procesamiento (CPU) y una Unidad de Procesamiento Gr√°fico (GPU). Este repositorio no es solo te√≥rico; lo utilizo como mi bit√°cora para aplicar estos conceptos en proyectos de c√≥mputo de alto rendimiento.

---

## üéØ Objetivos del Proyecto 

El prop√≥sito de este repositorio es doble:

1.  **Documentar mi aprendizaje:** Servir como una referencia personal detallada sobre la arquitectura de c√≥mputo paralelo.
2.  **Aplicar el conocimiento:** Implementar y optimizar algoritmos que demuestren el poder de la aceleraci√≥n por GPU. Espec√≠ficamente, me enfocar√© en dos √°reas clave:
    * **Simulaci√≥n de Monte Carlo:** Desarrollar√© una simulaci√≥n para analizar c√≥mo el paralelismo masivo de una GPU puede acelerar dr√°sticamente los m√©todos estoc√°sticos.
    * **Machine Learning en Medicina:** Abordar√© un problema del sector salud, aplicando t√©cnicas de machine learning donde la velocidad de procesamiento de grandes vol√∫menes de datos es cr√≠tica.

---



## üêõ El Desaf√≠o: Conflictos de Entorno (PyTorch vs. Numba)

Durante el desarrollo inicial, **descubr√≠ un conflicto de dependencias fundamental** al intentar instalar `numba` (que requiere un `cudatoolkit` espec√≠fico de `conda-forge`) junto con `pytorch` (que impone su propio paquete `pytorch-cuda` y bibliotecas asociadas).

Conda no pudo resolver una configuraci√≥n compatible que satisficiera ambos paquetes simult√°neamente en el mismo entorno. Esto llev√≥ a errores persistentes como `LibMambaUnsatisfiableError` o `FileNotFoundError: nvvm.dll`.

**üí° La Soluci√≥n Adoptada:** La estrategia m√°s robusta y que recomiendo es mantener **entornos Conda separados**:

1.  Un entorno dedicado exclusivamente a **Numba y c√≥digo CUDA** (el descrito en este `README`, necesario para ejecutar este proyecto).
2.  Otro entorno separado para **PyTorch** (para proyectos futuros de Machine Learning).

---
## üöÄ Configuraci√≥n del Entorno (Numba CUDA para Este Proyecto)

Para ejecutar el c√≥digo de este repositorio (`Montecarlo_cuda.ipynb`), es **esencial** crear y configurar un entorno Conda espec√≠fico para Numba de la siguiente manera:

### Requisitos Previos

1.  **GPU NVIDIA:** Indispensable contar con una tarjeta gr√°fica NVIDIA compatible con CUDA.
2.  **NVIDIA CUDA Toolkit (Instalaci√≥n del Sistema):** Aunque Conda instalar√° sus propias bibliotecas, recomiendo tener el CUDA Toolkit oficial instalado en el sistema (descargado desde [NVIDIA Developer](https://developer.nvidia.com/cuda-toolkit)). Esto asegura que los *drivers* de la GPU est√©n actualizados. La instalaci√≥n de Conda manejar√° las bibliotecas necesarias para Numba.
3.  **Miniconda/Anaconda:** Necesitas una instalaci√≥n de Conda.

### Pasos de Configuraci√≥n

4. Crear el Entorno Conda (`env_numba`):

Abre tu terminal (Anaconda Prompt) y ejecuta este comando. Usamos `conda-forge` como canal prioritario para asegurar la compatibilidad:  

conda create -n env_numba -c conda-forge python=3.10 numba cudatoolkit jupyterlab ipykernel numpy

- `-n env_numba`: Nombre del entorno.
- `-c conda-forge`: Prioriza este canal. Es crucial para la compatibilidad Numba/cudatoolkit.
- `python=3.10`: Versi√≥n de Python (ajusta si es necesario).
- `numba cudatoolkit`: Instala Numba y las bibliotecas CUDA correspondientes.
- `jupyterlab ipykernel numpy`: Herramientas para ejecutar el notebook.

5. Activar el Entorno:

Cada vez que trabajes en este proyecto, activa el entorno:

conda activate env_numba

(Tu terminal deber√≠a mostrar `(env_numba)` al inicio).

 6. Conectar el Entorno a Jupyter

Para que Jupyter Notebook/Lab reconozca este entorno, reg√≠stralo como un "Kernel":
python -m ipykernel install --user --name="env_numba" --display-name="Python (Numba Only)"

 `--display-name="Python (Numba Only)"`: Este es el nombre que ver√°s en Jupyter.

---
## ‚ñ∂Ô∏è C√≥mo Ejecutar el Notebook

1. Clona este repositorio a tu m√°quina local.
2. Abre tu terminal (Anaconda Prompt).
3. Crea el entorno (`env_numba`)
4. Activa el entorno Conda: `conda activate env_numba`.
6. Navega a la carpeta donde clonaste el repositorio.
7. Inicia JupyterLab: `jupyter-lab`.
8. Abre el notebook `Montecarlo_cuda.ipynb`.
9. **¬°Verifica y Selecciona el Kernel Correcto!** Dentro de Jupyter, ve al men√∫ **Kernel > Change Kernel...** y aseg√∫rate de seleccionar **"Python (Numba Only)"**. Si est√° seleccionado otro, c√°mbialo.
10. Ejecuta las celdas del notebook en orden.




---

## üß† El Dilema Principal: Latencia vs. Rendimiento

Mi conclusi√≥n clave es que toda la arquitectura se reduce a una filosof√≠a de dise√±o:

* **CPU (Dise√±o Orientado a la Latencia):** Una CPU est√° optimizada para ser extremadamente r√°pida en tareas **secuenciales**. Su objetivo es minimizar la latencia, es decir, el tiempo que tarda en completar una sola tarea.

* **GPU (Dise√±o Orientado al Rendimiento):** Una GPU est√° optimizada para el **rendimiento** (throughput). Su objetivo es ejecutar una cantidad masiva de tareas **en paralelo**, incluso si cada tarea individual tarda un poco m√°s.

---

## üèõÔ∏è Diferencias Arquitect√≥nicas

La filosof√≠a de dise√±o se refleja directamente en el hardware.

### La CPU: El Especialista Veloz

Una CPU tiene unos pocos n√∫cleos (cores) que son incre√≠blemente potentes. Gran parte del espacio f√≠sico del chip se dedica a:

* **L√≥gica de Control (Control):** Unidades complejas para manejar la ejecuci√≥n de instrucciones de manera eficiente.
* **Cach√© (Cache):** Grandes cantidades de memoria cach√© para tener los datos listos instant√°neamente y no tener que esperar a la memoria principal (DRAM).

Esto la hace ideal para c√≥digo secuencial, donde cada paso depende del anterior.

### La GPU: El Ej√©rcito Paralelo

Una GPU adopta el enfoque opuesto. Dedica la gran mayor√≠a de su espacio f√≠sico a la computaci√≥n pura:

* **ALU (Unidades Aritm√©tico-L√≥gicas):** Cientos o miles de n√∫cleos de procesamiento m√°s simples dise√±ados para ejecutar la misma instrucci√≥n sobre diferentes datos (paralelismo).
* **Menos Cach√© y Control:** Tiene unidades de control y cach√© mucho m√°s simples en comparaci√≥n con una CPU.

Una GPU est√° dise√±ada para ejecutar miles de hilos (threads) simult√°neamente. En el modelo CUDA, estos hilos se organizan en bloques (blocks) y cuadr√≠culas (grids), permitiendo que una sola instrucci√≥n opere sobre miles de puntos de datos a la vez.

---

## ü§ù Cu√°ndo Usar Cada Uno: C√≥mputo Heterog√©neo

En la pr√°ctica, las aplicaciones m√°s eficientes no eligen entre una u otra, sino que utilizan ambas. Este modelo se conoce como **c√≥mputo heterog√©neo**.

En mi trabajo y estudio, he visto este patr√≥n:

1.  **La CPU act√∫a como el "Host":** Maneja la l√≥gica general de la aplicaci√≥n, las tareas secuenciales y la E/S. Prepara los datos.
2.  **Transferencia de Datos:** La CPU copia los datos necesarios del host (su memoria RAM) al "dispositivo" (la memoria de la GPU).
3.  **La GPU act√∫a como el "Dispositivo":** La CPU lanza un *kernel* (una funci√≥n de la GPU) para que se ejecute en el dispositivo. La GPU entonces procesa todos los datos en paralelo masivo, logrando una aceleraci√≥n de 10X o m√°s en estas partes del c√≥digo.
4.  **Sincronizaci√≥n y Resultados:** La CPU puede continuar con otras tareas y luego esperar (sincronizar) a que la GPU termine. Finalmente, copia los resultados de vuelta a la memoria del host para su verificaci√≥n o uso posterior.

En resumen, la CPU es el director de orquesta que maneja la l√≥gica compleja y secuencial, mientras que la GPU es la orquesta completa que ejecuta las partes de trabajo pesado y masivamente paralelas.

---

## üìú Licencia

El c√≥digo fuente de este proyecto ( `.ipynb`) se distribuye bajo la **Licencia MIT**.

El software de terceros del que depende (como el NVIDIA CUDA Toolkit) mantiene sus propias licencias y acuerdos (EULA).
