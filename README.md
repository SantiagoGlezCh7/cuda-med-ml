# Entendiendo las Diferencias Fundamentales: CPU vs. GPU

Este documento resume mis notas y conclusiones sobre las diferencias arquitect√≥nicas y de casos de uso entre una Unidad Central de Procesamiento (CPU) y una Unidad de Procesamiento Gr√°fico (GPU). Este repositorio no es solo te√≥rico; lo utilizo como mi bit√°cora para aplicar estos conceptos en proyectos de c√≥mputo de alto rendimiento.

---

## üéØ Objetivos del Proyecto

El prop√≥sito de este repositorio es doble:

1.  **Documentar mi aprendizaje:** Servir como una referencia personal detallada sobre la arquitectura de c√≥mputo paralelo.
2.  **Aplicar el conocimiento:** Implementar y optimizar algoritmos que demuestren el poder de la aceleraci√≥n por GPU. Espec√≠ficamente, me enfocar√© en dos √°reas clave:
    * **Simulaci√≥n de Monte Carlo:** Desarrollar√© una simulaci√≥n para analizar c√≥mo el paralelismo masivo de una GPU puede acelerar dr√°sticamente los m√©todos estoc√°sticos.
    * **Machine Learning en Medicina:** Abordar√© un problema del sector salud, aplicando t√©cnicas de machine learning donde la velocidad de procesamiento de grandes vol√∫menes de datos es cr√≠tica.

---

## üöÄ Requisitos e Instalaci√≥n

Para explorar los ejemplos o ejecutar el c√≥digo de este repositorio, necesitar√°s un entorno espec√≠fico que combine hardware y software.

### Requisitos Previos de Hardware y Software

1.  **GPU NVIDIA:** Es indispensable contar con una tarjeta gr√°fica NVIDIA compatible con CUDA.
2.  **NVIDIA CUDA Toolkit:** Este proyecto *depende* del NVIDIA CUDA Toolkit. Deber√°s descargarlo e instalarlo por separado desde el [sitio oficial de NVIDIA Developer](https://developer.nvidia.com/cuda-toolkit).
    * La instalaci√≥n y uso del CUDA Toolkit est√°n sujetos al Acuerdo de Licencia de Usuario Final (EULA) de NVIDIA.
3.  **Python:** Se recomienda Python 3.8 o superior.

### Configuraci√≥n del Entorno

1.  **Jupyter Notebook:** Los an√°lisis y ejemplos de este proyecto est√°n presentados en formato de Jupyter Notebooks (`.ipynb`). Es la herramienta que he elegido para ejecutar y visualizar el c√≥digo interactivamente.
    * Puedes instalar Jupyter Notebook usando:
      ```bash
      pip install jupyterlab
      ```
2.  **Bibliotecas de Python:** El c√≥digo utiliza Numba para la compilaci√≥n JIT (Just-in-Time) de CUDA Python, junto con otras bibliotecas est√°ndar.
    * Instala las bibliotecas necesarias:
      ```bash
      pip install numba numpy pandas
      ```
3.  **Ejecuci√≥n:**
    * Clona este repositorio.
    * Navega a la carpeta del proyecto y ejecuta:
      ```bash
      jupyter-lab
      ```
    * Esto abrir√° JupyterLab en tu navegador, permiti√©ndote abrir y ejecutar los notebooks.

---

## üß† El Dilema Principal: Latencia vs. Rendimiento

Mi conclusi√≥n clave es que toda la arquitectura se reduce a una filosof√≠a de dise√±o:

* **CPU (Dise√±o Orientado a la Latencia):** Una CPU est√° optimizada para ser extremadamente r√°pida en tareas **secuenciales**. Su objetivo es minimizar la latencia, es decir, el tiempo que tarda en completar una sola tarea.

* **GPU (Dise√±o Orientado al Rendimiento):** Una GPU est√° optimizada para el **rendimiento** (throughput). Su objetivo es ejecutar una cantidad masiva de tareas **en paralelo**, incluso si cada tarea individual tarda un poco m√°s.

---

## üèõÔ∏è Diferencias Arquitect√≥nicas

La filosof√≠a de dise√±o se refleja directamente en el hardware.

### La CPU: El Especialista Veloz

Una CPU tiene unos pocos n√∫cleos (cores) que son incre√≠blemente potentes. Gran parte del espacio f√≠sico del chip se dedica a:

* **L√≥gica de Control (Control):** Unidades complejas para manejar la ejecuci√≥n de instrucciones de manera eficiente.
* **Cach√© (Cache):** Grandes cantidades de memoria cach√© para tener los datos listos instant√°neamente y no tener que esperar a la memoria principal (DRAM).

Esto la hace ideal para c√≥digo secuencial, donde cada paso depende del anterior.

### La GPU: El Ej√©rcito Paralelo

Una GPU adopta el enfoque opuesto. Dedica la gran mayor√≠a de su espacio f√≠sico a la computaci√≥n pura:

* **ALU (Unidades Aritm√©tico-L√≥gicas):** Cientos o miles de n√∫cleos de procesamiento m√°s simples dise√±ados para ejecutar la misma instrucci√≥n sobre diferentes datos (paralelismo).
* **Menos Cach√© y Control:** Tiene unidades de control y cach√© mucho m√°s simples en comparaci√≥n con una CPU.

Una GPU est√° dise√±ada para ejecutar miles de hilos (threads) simult√°neamente. En el modelo CUDA, estos hilos se organizan en bloques (blocks) y cuadr√≠culas (grids), permitiendo que una sola instrucci√≥n opere sobre miles de puntos de datos a la vez.

---

## ü§ù Cu√°ndo Usar Cada Uno: C√≥mputo Heterog√©neo

En la pr√°ctica, las aplicaciones m√°s eficientes no eligen entre una u otra, sino que utilizan ambas. Este modelo se conoce como **c√≥mputo heterog√©neo**.

En mi trabajo y estudio, he visto este patr√≥n:

1.  **La CPU act√∫a como el "Host":** Maneja la l√≥gica general de la aplicaci√≥n, las tareas secuenciales y la E/S. Prepara los datos.
2.  **Transferencia de Datos:** La CPU copia los datos necesarios del host (su memoria RAM) al "dispositivo" (la memoria de la GPU).
3.  **La GPU act√∫a como el "Dispositivo":** La CPU lanza un *kernel* (una funci√≥n de la GPU) para que se ejecute en el dispositivo. La GPU entonces procesa todos los datos en paralelo masivo, logrando una aceleraci√≥n de 10X o m√°s en estas partes del c√≥digo.
4.  **Sincronizaci√≥n y Resultados:** La CPU puede continuar con otras tareas y luego esperar (sincronizar) a que la GPU termine. Finalmente, copia los resultados de vuelta a la memoria del host para su verificaci√≥n o uso posterior.

En resumen, la CPU es el director de orquesta que maneja la l√≥gica compleja y secuencial, mientras que la GPU es la orquesta completa que ejecuta las partes de trabajo pesado y masivamente paralelas.

---

## üìú Licencia

El c√≥digo fuente de este proyecto (los archivos `.py` y `.ipynb`) se distribuye bajo la **Licencia MIT**.

El software de terceros del que depende (como el NVIDIA CUDA Toolkit) mantiene sus propias licencias y acuerdos (EULA).
